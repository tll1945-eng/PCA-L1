 # Create a random matrix of dimension DxN.
# 	X = np.random.randn(D, N)
# U, S, V = torch.svd_lowrank(X, q=512, niter=16)

import torch
import torch.nn.functional as F

import pandas as pd
import time

import numpy as np




	
def pca_l1_each_svd_iterate_w(data,pca_dim):
    start_time = time.time()
    svd_total_time = 0


    # 2. 转换为PyTorch Tensor，并移动到GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"使用设备: {device}")

    # 关键步骤：转换为Tensor并指定设备
    print(f"X的设备: {data.device}")
    X = data.to(device)
    print(f"X的设备: {X.device}")  # 应显示 'cuda:0'


    def interate_w(X):
        nonlocal svd_total_time

        svd_start_time = time.time()

        U, S, Vh = torch.linalg.svd(X, full_matrices=False)

        svd_end_time = time.time()
        svd_elapsed_time = svd_end_time - svd_start_time
        svd_total_time = svd_total_time  + svd_elapsed_time 
        print(f"svd_total_time: {svd_total_time}")

        normalized_w_col = U[:,0:1]  
        
        euclidean_dist = 1.0
        threshold = 1e-6

        while euclidean_dist > threshold:        
            normalized_old_w_col = normalized_w_col.clone()
            
            outer_product_row = normalized_w_col.T @ X 
            signs_col = torch.sign(outer_product_row).T
            w_col =  X @ signs_col.float()
            normalized_w_col = F.normalize(w_col, dim=0, p=2)
            euclidean_dist = torch.norm(normalized_w_col - normalized_old_w_col, p=2)
            
        
        return normalized_w_col




    

    principal_list = []

    for i in range(pca_dim):
        normalized_w_col = interate_w(X)
        principal_list.append(normalized_w_col)
        tmp1_row = normalized_w_col.T @ X
        X = X - normalized_w_col @ tmp1_row
    # print("principals:", principals)
    principal_matrix = torch.cat(principal_list, dim=1)  # 沿第二维度（列方向）拼接

    print("principals 矩阵形状:", principal_matrix.shape)
    print("principals 矩阵:", principal_matrix)

    

    end_time = time.time()
    elapsed_time = end_time - start_time
    svd_pro = svd_total_time / elapsed_time
    print(f"algorithm 2执行时间: {elapsed_time:.6f} 秒")
    print(f" proportion of svd:{svd_pro}")

    return principal_matrix  # 返回矩阵而不是列表
    
#########################################################################
def iter_w_with_new_l2_pc(X,normalized_w_col):
      
        
        euclidean_dist = 1.0
        threshold = 1e-6

        while euclidean_dist > threshold:        
            normalized_old_w_col = normalized_w_col.clone()
            
            outer_product_row = normalized_w_col.T @ X 
            signs_col = torch.sign(outer_product_row).T
            w_col =  X @ signs_col.float()
            normalized_w_col = F.normalize(w_col, dim=0, p=2)
            euclidean_dist = torch.norm(normalized_w_col - normalized_old_w_col, p=2)
            
        
        return normalized_w_col




def pca_l1_only_once_svd(data,pca_dim):
    # start_time = time.time()
    # svd_total_time = 0


    # 2. 转换为PyTorch Tensor，并移动到GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"使用设备: {device}")

    # 关键步骤：转换为Tensor并指定设备
    print(f"X的设备: {data.device}")
    X = data.to(device)
    print(f"X的设备: {X.device}")  # 应显示 'cuda:0'

    U, S, Vh = torch.linalg.svd(X, full_matrices=False)
    new_l2_pc_col = U[:,0:1] 
    normalized_w_col = new_l2_pc_col 

    



    

    principal_list = []

    for i in range(pca_dim):
        normalized_w_col = normalized_w_col.clone()
        normalized_w_col = iter_w_with_new_l2_pc(X,normalized_w_col)
        principal_list.append(normalized_w_col)
        tmp1_row = normalized_w_col.T @ X
        X = X - normalized_w_col @ tmp1_row
        new_l2_pc_col = new_l2_pc_col - normalized_w_col
   
    principal_matrix = torch.cat(principal_list, dim=1)  # 沿第二维度（列方向）拼接

    print("principals 矩阵形状:", principal_matrix.shape)
    print("principals 矩阵:", principal_matrix)

    

    # end_time = time.time()
    # elapsed_time = end_time - start_time
    # svd_pro = svd_total_time / elapsed_time
    # print(f"algorithm 2执行时间: {elapsed_time:.6f} 秒")
    # print(f" proportion of svd:{svd_pro}")

    # return principal_matrix  # 返回矩阵而不是列表
    pass
    



if __name__ == "__main__":
    # data = torch.rand(4096, 4096)
    # pca_l1_each_svd_iterate_w(data,16)   
    df = pd.read_csv("/root/USArrests_subset.csv", index_col=0)
    print("data dimension:", df.shape) 
    print("head of data:") 
    print(df.head())

    #matrix_numpy 特征×样本 与论文Principal Component Analysis Based on L1-Norm Maximization一致
    data =  torch.from_numpy(df.values.T).float() # 此时是NumPy数组，在CPU上
    
    pca_dim = 2
    pca_l1_only_once_svd(data,pca_dim)     
    print("ok")

        